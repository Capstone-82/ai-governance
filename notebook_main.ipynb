{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bdb832",
   "metadata": {},
   "source": [
    "# AWS Bedrock Access Validation - Governance First\n",
    "\n",
    "## Purpose\n",
    "This notebook validates AWS Bedrock access with a governance-first approach, capturing telemetry required for:\n",
    "- Cost attribution and FinOps\n",
    "- Compliance auditing\n",
    "- Provider SLA monitoring\n",
    "- Capacity planning\n",
    "\n",
    "## Prerequisites\n",
    "- AWS credentials configured (environment variables, ~/.aws/credentials, or IAM role)\n",
    "- Bedrock access enabled in your AWS account\n",
    "- Bedrock model access granted (specifically Meta Llama models)\n",
    "\n",
    "## Governance Principles\n",
    "1. **Identity verification** - Know who/what is making API calls\n",
    "2. **Telemetry capture** - All invocations must be measurable\n",
    "3. **Failure classification** - Distinguish billing, permission, and availability issues\n",
    "4. **Audit readiness** - Structured logging for compliance reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any, Optional\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d2d2f",
   "metadata": {},
   "source": [
    "## A. Authentication & Identity\n",
    "\n",
    "### IAM Authentication Model\n",
    "AWS Bedrock uses IAM (Identity and Access Management) for authentication and authorization:\n",
    "- **Authentication**: Verified via AWS credentials (access keys, IAM roles, or temporary credentials)\n",
    "- **Authorization**: Controlled by IAM policies attached to the identity\n",
    "- **Required permissions**: `bedrock:InvokeModel`, `bedrock:ListFoundationModels`\n",
    "\n",
    "### Identity Validation\n",
    "We use AWS STS (Security Token Service) to validate the calling identity before any Bedrock operations.\n",
    "This ensures:\n",
    "- Credentials are valid\n",
    "- We know which principal is being charged\n",
    "- Audit trails are attributable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e914b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AWS Identity Validated\n",
      "  Account: 108839616732\n",
      "  ARN: arn:aws:iam::108839616732:user/Hamdan\n",
      "  UserId: AIDARSV2YYDOJNXPVBBFF\n"
     ]
    }
   ],
   "source": [
    "# Validate calling identity using STS\n",
    "def validate_identity() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate AWS credentials and return calling identity information.\n",
    "    Critical for governance: establishes accountability for API usage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sts_client = boto3.client('sts')\n",
    "        identity = sts_client.get_caller_identity()\n",
    "        \n",
    "        print(\"✓ AWS Identity Validated\")\n",
    "        print(f\"  Account: {identity['Account']}\")\n",
    "        print(f\"  ARN: {identity['Arn']}\")\n",
    "        print(f\"  UserId: {identity['UserId']}\")\n",
    "        \n",
    "        return identity\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Identity Validation Failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute validation\n",
    "identity = validate_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f756a7",
   "metadata": {},
   "source": [
    "## B. API / Runtime Validation\n",
    "\n",
    "### Bedrock Runtime Architecture\n",
    "- **Control Plane**: `bedrock` client - lists models, manages access\n",
    "- **Data Plane**: `bedrock-runtime` client - invokes models\n",
    "\n",
    "### Model Selection\n",
    "We target Meta Llama models (Instruct variants) for:\n",
    "- Open-source governance alignment\n",
    "- Predictable licensing\n",
    "- Strong instruction-following capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b27afa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 11 models (filtered by: meta)\n",
      "  - meta.llama3-8b-instruct-v1:0\n",
      "    Provider: Meta\n",
      "    Inference: ON_DEMAND\n",
      "  - meta.llama3-70b-instruct-v1:0\n",
      "    Provider: Meta\n",
      "    Inference: ON_DEMAND\n",
      "  - meta.llama3-1-8b-instruct-v1:0\n",
      "    Provider: Meta\n",
      "    Inference: INFERENCE_PROFILE\n",
      "  - meta.llama3-1-70b-instruct-v1:0\n",
      "    Provider: Meta\n",
      "    Inference: INFERENCE_PROFILE\n",
      "  - meta.llama3-2-11b-instruct-v1:0\n",
      "    Provider: Meta\n",
      "    Inference: INFERENCE_PROFILE\n",
      "  ... and 6 more\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelArn</th>\n",
       "      <th>modelId</th>\n",
       "      <th>modelName</th>\n",
       "      <th>providerName</th>\n",
       "      <th>inputModalities</th>\n",
       "      <th>outputModalities</th>\n",
       "      <th>responseStreamingSupported</th>\n",
       "      <th>customizationsSupported</th>\n",
       "      <th>inferenceTypesSupported</th>\n",
       "      <th>modelLifecycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-8b-instruct-v1:0</td>\n",
       "      <td>Llama 3 8B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-70b-instruct-v1:0</td>\n",
       "      <td>Llama 3 70B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-1-8b-instruct-v1:0</td>\n",
       "      <td>Llama 3.1 8B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-1-70b-instruct-v1:0</td>\n",
       "      <td>Llama 3.1 70B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-2-11b-instruct-v1:0</td>\n",
       "      <td>Llama 3.2 11B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-2-90b-instruct-v1:0</td>\n",
       "      <td>Llama 3.2 90B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-2-1b-instruct-v1:0</td>\n",
       "      <td>Llama 3.2 1B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-2-3b-instruct-v1:0</td>\n",
       "      <td>Llama 3.2 3B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama3-3-70b-instruct-v1:0</td>\n",
       "      <td>Llama 3.3 70B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama4-scout-17b-instruct-v1:0</td>\n",
       "      <td>Llama 4 Scout 17B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama4-maverick-17b-instruct-v1:0</td>\n",
       "      <td>Llama 4 Maverick 17B Instruct</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[INFERENCE_PROFILE]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             modelArn  \\\n",
       "0   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "1   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "2   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "3   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "4   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "5   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "6   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "7   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "8   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "9   arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "10  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "\n",
       "                                   modelId                      modelName  \\\n",
       "0             meta.llama3-8b-instruct-v1:0            Llama 3 8B Instruct   \n",
       "1            meta.llama3-70b-instruct-v1:0           Llama 3 70B Instruct   \n",
       "2           meta.llama3-1-8b-instruct-v1:0          Llama 3.1 8B Instruct   \n",
       "3          meta.llama3-1-70b-instruct-v1:0         Llama 3.1 70B Instruct   \n",
       "4          meta.llama3-2-11b-instruct-v1:0         Llama 3.2 11B Instruct   \n",
       "5          meta.llama3-2-90b-instruct-v1:0         Llama 3.2 90B Instruct   \n",
       "6           meta.llama3-2-1b-instruct-v1:0          Llama 3.2 1B Instruct   \n",
       "7           meta.llama3-2-3b-instruct-v1:0          Llama 3.2 3B Instruct   \n",
       "8          meta.llama3-3-70b-instruct-v1:0         Llama 3.3 70B Instruct   \n",
       "9      meta.llama4-scout-17b-instruct-v1:0     Llama 4 Scout 17B Instruct   \n",
       "10  meta.llama4-maverick-17b-instruct-v1:0  Llama 4 Maverick 17B Instruct   \n",
       "\n",
       "   providerName inputModalities outputModalities  responseStreamingSupported  \\\n",
       "0          Meta          [TEXT]           [TEXT]                        True   \n",
       "1          Meta          [TEXT]           [TEXT]                        True   \n",
       "2          Meta          [TEXT]           [TEXT]                        True   \n",
       "3          Meta          [TEXT]           [TEXT]                        True   \n",
       "4          Meta   [TEXT, IMAGE]           [TEXT]                        True   \n",
       "5          Meta   [TEXT, IMAGE]           [TEXT]                        True   \n",
       "6          Meta          [TEXT]           [TEXT]                        True   \n",
       "7          Meta          [TEXT]           [TEXT]                        True   \n",
       "8          Meta          [TEXT]           [TEXT]                        True   \n",
       "9          Meta   [TEXT, IMAGE]           [TEXT]                        True   \n",
       "10         Meta   [TEXT, IMAGE]           [TEXT]                        True   \n",
       "\n",
       "   customizationsSupported inferenceTypesSupported        modelLifecycle  \n",
       "0                       []             [ON_DEMAND]  {'status': 'ACTIVE'}  \n",
       "1                       []             [ON_DEMAND]  {'status': 'ACTIVE'}  \n",
       "2                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "3                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "4                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "5                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "6                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "7                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "8                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "9                       []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  \n",
       "10                      []     [INFERENCE_PROFILE]  {'status': 'ACTIVE'}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available Bedrock foundation models\n",
    "def list_available_models(provider_filter: Optional[str] = \"meta\") -> list:\n",
    "    \"\"\"\n",
    "    Enumerate available models in Bedrock.\n",
    "    Governance value: Verify entitlements and discover new models.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bedrock_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "        response = bedrock_client.list_foundation_models()\n",
    "        \n",
    "        models = response.get('modelSummaries', [])\n",
    "        \n",
    "        if provider_filter:\n",
    "            models = [m for m in models if provider_filter.lower() in m.get('providerName', '').lower()]\n",
    "        \n",
    "        print(f\"✓ Found {len(models)} models\" + (f\" (filtered by: {provider_filter})\" if provider_filter else \"\"))\n",
    "        \n",
    "        for model in models[:5]:  # Show first 5\n",
    "            print(f\"  - {model['modelId']}\")\n",
    "            print(f\"    Provider: {model.get('providerName', 'N/A')}\")\n",
    "            print(f\"    Inference: {', '.join(model.get('inferenceTypesSupported', []))}\")\n",
    "        \n",
    "        if len(models) > 5:\n",
    "            print(f\"  ... and {len(models) - 5} more\")\n",
    "        \n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model Listing Failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute model discovery\n",
    "df = pd.DataFrame(list_available_models(provider_filter=\"meta\"))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e09c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: us.meta.llama3-2-1b-instruct-v1:0\n",
      "Note: Adjust MODEL_ID if this model is unavailable in your account\n"
     ]
    }
   ],
   "source": [
    "# Select a Meta Llama model for invocation\n",
    "# Using Llama 3.1 8B Instruct as default (cost-effective, widely available)\n",
    "MODEL_ID = \"us.meta.llama3-2-1b-instruct-v1:0\"  # Adjust based on your region/access\n",
    "\n",
    "print(f\"Selected Model: {MODEL_ID}\")\n",
    "print(\"Note: Adjust MODEL_ID if this model is unavailable in your account\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5678c4f",
   "metadata": {},
   "source": [
    "## AI Model Metadata & Governance Tagging\n",
    "\n",
    "Before provisioning or invoking any AI model, governance platforms require\n",
    "explicit **model metadata and tagging** to enable cost attribution, policy\n",
    "enforcement, and auditability.\n",
    "\n",
    "Each model invocation is classified using the following governance tags:\n",
    "\n",
    "- Provider (e.g., aws_bedrock, openai)\n",
    "- Model ID (exact model identifier)\n",
    "- Model Class (reasoning / general / lightweight)\n",
    "- Risk Tier (low / medium / high)\n",
    "- Cost Tier (budget / standard / premium)\n",
    "\n",
    "These tags are captured at invocation time and form the basis for:\n",
    "- FinOps cost allocation\n",
    "- Policy enforcement\n",
    "- Usage analytics\n",
    "- Cross-cloud normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cdeb7",
   "metadata": {},
   "source": [
    "## C. Model Invocation with Governance Telemetry\n",
    "\n",
    "### Why These Metrics Matter\n",
    "\n",
    "| Metric | Governance Value |\n",
    "|--------|------------------|\n",
    "| **provider** | Vendor risk management, multi-cloud strategy |\n",
    "| **model_id** | Cost allocation, performance benchmarking |\n",
    "| **latency_ms** | SLA compliance, user experience tracking |\n",
    "| **token_usage** | FinOps (primary cost driver), capacity planning |\n",
    "| **timestamp** | Audit trails, usage pattern analysis |\n",
    "| **failure_classification** | Operational health, cost avoidance (detect quota issues) |\n",
    "\n",
    "### Token Usage Caveat\n",
    "Not all Bedrock models return token counts in responses. This is a **provider limitation**, not a notebook issue.\n",
    "For governance: Log as `null` when unavailable, track separately via CloudWatch if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abe6fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Governance telemetry structure\n",
    "class InvocationTelemetry:\n",
    "    \"\"\"\n",
    "    Structured telemetry for AI governance.\n",
    "    Designed for export to SIEM, data warehouse, or FinOps platforms.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.provider = None\n",
    "        self.model_id = None\n",
    "        self.latency_ms = None\n",
    "        self.input_tokens = None\n",
    "        self.output_tokens = None\n",
    "        self.timestamp = None\n",
    "        self.success = False\n",
    "        self.failure_classification = None\n",
    "        self.error_message = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"provider\": self.provider,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"latency_ms\": self.latency_ms,\n",
    "            \"input_tokens\": self.input_tokens,\n",
    "            \"output_tokens\": self.output_tokens,\n",
    "            \"total_tokens\": (self.input_tokens + self.output_tokens) if (self.input_tokens and self.output_tokens) else None,\n",
    "            \"timestamp\": self.timestamp,\n",
    "            \"success\": self.success,\n",
    "            \"failure_classification\": self.failure_classification,\n",
    "            \"error_message\": self.error_message\n",
    "        }\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"Print telemetry in audit-ready format\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GOVERNANCE TELEMETRY\")\n",
    "        print(\"=\"*60)\n",
    "        for key, value in self.to_dict().items():\n",
    "            print(f\"{key:.<30} {value}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize telemetry collector\n",
    "telemetry = InvocationTelemetry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bc0d5",
   "metadata": {},
   "source": [
    "## D. Failure Handling & Classification\n",
    "\n",
    "### Failure Categories\n",
    "- **PERMISSION_DENIED**: IAM policy issues, model access not granted\n",
    "- **THROTTLING**: Rate limits, quota exhaustion (critical for cost management)\n",
    "- **VALIDATION_ERROR**: Malformed request (code issue, not governance issue)\n",
    "- **SERVICE_ERROR**: Provider-side failures\n",
    "- **UNKNOWN**: Unclassified errors (requires investigation)\n",
    "\n",
    "### Governance Implications\n",
    "- **Throttling** → Indicates capacity planning needed or unexpected usage spike\n",
    "- **Permission errors** → Audit IAM policies, verify entitlements\n",
    "- **Service errors** → Track for SLA compliance, consider multi-provider strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9bc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_failure(error: Exception) -> str:\n",
    "    \"\"\"\n",
    "    Classify failures for governance reporting.\n",
    "    Maps AWS exceptions to governance-relevant categories.\n",
    "    \"\"\"\n",
    "    error_str = str(error)\n",
    "    error_type = type(error).__name__\n",
    "    \n",
    "    # AWS Bedrock specific error patterns\n",
    "    if \"AccessDeniedException\" in error_type or \"UnauthorizedException\" in error_type:\n",
    "        return \"PERMISSION_DENIED\"\n",
    "    elif \"ThrottlingException\" in error_type or \"TooManyRequestsException\" in error_type:\n",
    "        return \"THROTTLING\"\n",
    "    elif \"ValidationException\" in error_type or \"invalid\" in error_str.lower():\n",
    "        return \"VALIDATION_ERROR\"\n",
    "    elif \"ServiceException\" in error_type or \"InternalServerError\" in error_type:\n",
    "        return \"SERVICE_ERROR\"\n",
    "    elif \"ModelNotReadyException\" in error_type:\n",
    "        return \"MODEL_UNAVAILABLE\"\n",
    "    else:\n",
    "        return \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model Invocation Successful\n",
      "  Latency: 5673.57ms\n",
      "  Response Length: 2112 characters\n",
      "\n",
      "============================================================\n",
      "GOVERNANCE TELEMETRY\n",
      "============================================================\n",
      "provider...................... aws_bedrock\n",
      "model_id...................... us.meta.llama3-2-1b-instruct-v1:0\n",
      "latency_ms.................... 5673.57\n",
      "input_tokens.................. 12\n",
      "output_tokens................. 379\n",
      "total_tokens.................. 391\n",
      "timestamp..................... 2026-02-01T12:46:28.774984+00:00\n",
      "success....................... True\n",
      "failure_classification........ None\n",
      "error_message................. None\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def invoke_llama_with_governance(prompt: str, model_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke Meta Llama model with full governance telemetry.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'response_text' and 'telemetry' keys\n",
    "    \"\"\"\n",
    "    telemetry = InvocationTelemetry()\n",
    "    telemetry.model_id = model_id\n",
    "    telemetry.provider = \"aws_bedrock\"\n",
    "    telemetry.timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    try:\n",
    "        bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "        \n",
    "        # Construct request payload for Llama models\n",
    "        request_body = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_gen_len\": 512,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "        \n",
    "        # Measure latency\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "            contentType='application/json',\n",
    "            accept='application/json'\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        telemetry.latency_ms = round((end_time - start_time) * 1000, 2)\n",
    "        \n",
    "        # Parse response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        \n",
    "        # Extract response text\n",
    "        response_text = response_body.get('generation', '')\n",
    "        \n",
    "        # Extract token usage (Meta Llama models may not always provide this)\n",
    "        telemetry.input_tokens = response_body.get('prompt_token_count')\n",
    "        telemetry.output_tokens = response_body.get('generation_token_count')\n",
    "        \n",
    "        # Handle missing token metrics (provider limitation)\n",
    "        if telemetry.input_tokens is None:\n",
    "            print(\"⚠ Warning: Input token count not provided by model (provider limitation)\")\n",
    "        if telemetry.output_tokens is None:\n",
    "            print(\"⚠ Warning: Output token count not provided by model (provider limitation)\")\n",
    "        \n",
    "        telemetry.success = True\n",
    "        \n",
    "        print(\"✓ Model Invocation Successful\")\n",
    "        print(f\"  Latency: {telemetry.latency_ms}ms\")\n",
    "        print(f\"  Response Length: {len(response_text)} characters\")\n",
    "        \n",
    "        return {\n",
    "            \"response_text\": response_text,\n",
    "            \"telemetry\": telemetry\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        telemetry.success = False\n",
    "        telemetry.failure_classification = classify_failure(e)\n",
    "        telemetry.error_message = str(e)\n",
    "        \n",
    "        print(f\"✗ Model Invocation Failed\")\n",
    "        print(f\"  Classification: {telemetry.failure_classification}\")\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "        \n",
    "        # Re-raise for caller to handle, but telemetry is captured\n",
    "        return {\n",
    "            \"response_text\": None,\n",
    "            \"telemetry\": telemetry,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Execute invocation with governance\n",
    "test_prompt = \"Explain the concept of cloud governance in exactly two sentences.\"\n",
    "\n",
    "result = invoke_llama_with_governance(\n",
    "    prompt=test_prompt,\n",
    "    model_id=MODEL_ID\n",
    ")\n",
    "\n",
    "# Display telemetry\n",
    "result['telemetry'].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0f11ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL RESPONSE\n",
      "============================================================\n",
      " Cloud governance refers to the processes and practices that organizations use to manage and govern their cloud-based resources, including data, applications, and infrastructure, to ensure efficient and secure use. Effective cloud governance involves setting clear policies, monitoring and managing cloud resources, and collaborating with cloud providers to ensure compliance with organizational standards and regulatory requirements.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display model response if successful\n",
    "if result['response_text']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL RESPONSE\")\n",
    "    print(\"=\"*60)\n",
    "    print(result['response_text'])\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n⚠ No response generated due to invocation failure\")\n",
    "    print(f\"Error: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8dfac",
   "metadata": {},
   "source": [
    "## OPEN AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d17b0c",
   "metadata": {},
   "source": [
    "## Authentication & Identity Model — OpenAI\n",
    "\n",
    "OpenAI uses a **static API key–based authentication model**.  \n",
    "Unlike cloud-native providers (AWS/GCP), OpenAI does **not provide native IAM, roles, or identity federation**.\n",
    "\n",
    "**Authentication Mechanism**\n",
    "- API Key passed via environment variable\n",
    "- No per-user or per-role identity context at the API level\n",
    "\n",
    "**Governance Implications**\n",
    "- API keys represent a shared identity and must be centrally managed\n",
    "- No native RBAC or policy enforcement\n",
    "- Requires an internal proxy or gateway layer for:\n",
    "  - Key rotation\n",
    "  - Per-user attribution\n",
    "  - Access control enforcement\n",
    "\n",
    "**Enterprise Consideration**\n",
    "OpenAI is easy to onboard but introduces **higher governance risk** without additional platform controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf337584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a gentle unicorn trotted through a sleeping meadow, curled up on a pillow of moss, and whispered goodnight to the stars.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadeca12",
   "metadata": {},
   "source": [
    "## Governance Telemetry Mapping\n",
    "\n",
    "Each OpenAI model invocation is captured using a **standardized governance telemetry schema**.\n",
    "\n",
    "**Captured Signals**\n",
    "- Provider: `openai`\n",
    "- Model identifier\n",
    "- Request latency (ms)\n",
    "- Input and output token counts\n",
    "- Invocation success or failure\n",
    "- Error classification (if applicable)\n",
    "\n",
    "**Why This Matters**\n",
    "Structured telemetry enables:\n",
    "- Cost attribution and FinOps analysis\n",
    "- Cross-provider normalization\n",
    "- Policy enforcement and auditability\n",
    "- Anomaly detection and usage forecasting\n",
    "\n",
    "This schema is intentionally aligned with telemetry collected from IAM-based providers (AWS/GCP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI Invocation Successful (gpt-3.5-turbo)\n",
      "\n",
      "============================================================\n",
      "GOVERNANCE TELEMETRY\n",
      "============================================================\n",
      "provider...................... openai\n",
      "model_id...................... gpt-3.5-turbo\n",
      "latency_ms.................... 1916.02\n",
      "input_tokens.................. 17\n",
      "output_tokens................. 31\n",
      "total_tokens.................. 48\n",
      "timestamp..................... 2026-02-01T13:49:47.634102+00:00\n",
      "success....................... True\n",
      "failure_classification........ None\n",
      "error_message................. None\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def invoke_openai_with_governance(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-4o\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Invoke OpenAI with governance telemetry adaptation.\n",
    "    Normalizes OpenAI metrics into our standard InvocationTelemetry format.\n",
    "    \"\"\"\n",
    "\n",
    "    telemetry = InvocationTelemetry()\n",
    "    telemetry.provider = \"openai\"\n",
    "    telemetry.model_id = model\n",
    "    telemetry.timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    try:\n",
    "        # Ensure API key is present\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "\n",
    "        end_time = time.time()\n",
    "        telemetry.latency_ms = round((end_time - start_time) * 1000, 2)\n",
    "\n",
    "        # Capture response text\n",
    "        response_text = response.choices[0].message.content\n",
    "\n",
    "        # Capture token usage\n",
    "        if response.usage:\n",
    "            telemetry.input_tokens = response.usage.prompt_tokens\n",
    "            telemetry.output_tokens = response.usage.completion_tokens\n",
    "\n",
    "        telemetry.success = True\n",
    "        print(f\"✓ OpenAI Invocation Successful ({model})\")\n",
    "\n",
    "        return {\n",
    "            \"response_text\": response_text,\n",
    "            \"telemetry\": telemetry\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        telemetry.success = False\n",
    "        telemetry.failure_classification = classify_failure(e)\n",
    "        telemetry.error_message = str(e)\n",
    "\n",
    "        print(f\"✗ OpenAI Invocation Failed: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"response_text\": None,\n",
    "            \"telemetry\": telemetry\n",
    "        }\n",
    "\n",
    "\n",
    "# Execute Governance Check\n",
    "openai_result = invoke_openai_with_governance(\n",
    "    prompt=\"Explain zero-trust security in one sentence.\",\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# Log Telemetry\n",
    "openai_result[\"telemetry\"].log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6e30c",
   "metadata": {},
   "source": [
    "## Governance Implications — OpenAI vs Cloud IAM Providers\n",
    "\n",
    "| Dimension | OpenAI | AWS Bedrock / GCP Vertex |\n",
    "|--------|--------|--------------------------|\n",
    "| Authentication | API key | IAM / Service Accounts |\n",
    "| Identity Context | Shared | Per-role / per-user |\n",
    "| Native RBAC | ❌  | ✅  |\n",
    "| Key Management Risk | High | Low |\n",
    "| Telemetry Quality | Strong (tokens) | Strong (infra + tokens) |\n",
    "\n",
    "**Conclusion**\n",
    "OpenAI requires **additional governance layers** (API gateways, identity proxies) to meet enterprise security and compliance standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model Invocation Successful\n",
      "  Latency: 7061.64ms\n",
      "  Response Length: 2892 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 7085.45ms\n",
      "  Response Length: 2310 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 6259.55ms\n",
      "  Response Length: 2358 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mopenai_cost\u001b[39m\u001b[33m\"\u001b[39m] = (df[\u001b[33m\"\u001b[39m\u001b[33mopenai_input_tokens\u001b[39m\u001b[33m\"\u001b[39m] / \u001b[32m1000\u001b[39m * \u001b[32m0.005\u001b[39m) + \\\n\u001b[32m     33\u001b[39m                     (df[\u001b[33m\"\u001b[39m\u001b[33mopenai_output_tokens\u001b[39m\u001b[33m\"\u001b[39m] / \u001b[32m1000\u001b[39m * \u001b[32m0.015\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Summary statistics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprovider\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.agg({\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlatency_ms\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m }))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Musharraf\\Documents\\Capstone'82\\ai-cloud-governance\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9213\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9216\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Musharraf\\Documents\\Capstone'82\\ai-cloud-governance\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Musharraf\\Documents\\Capstone'82\\ai-cloud-governance\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'provider'"
     ]
    }
   ],
   "source": [
    "# Run your Week 1 notebook with benchmark prompts\n",
    "benchmark_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a Python function to merge sorted lists.\",\n",
    "    \"Summarize the last 5 years of AI research.\",\n",
    "    # Add 7-17 more varied prompts\n",
    "]\n",
    "\n",
    "# Collect telemetry\n",
    "results = []\n",
    "for prompt in benchmark_prompts:\n",
    "    # AWS Bedrock\n",
    "    bedrock_result = invoke_llama_with_governance(prompt, MODEL_ID)\n",
    "    \n",
    "    # OpenAI\n",
    "    openai_result = invoke_openai_with_governance(prompt, model=\"gpt-4o\")\n",
    "    \n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"bedrock_latency_ms\": bedrock_result[\"telemetry\"].latency_ms,\n",
    "        \"bedrock_input_tokens\": bedrock_result[\"telemetry\"].input_tokens,\n",
    "        \"bedrock_output_tokens\": bedrock_result[\"telemetry\"].output_tokens,\n",
    "        \"openai_latency_ms\": openai_result[\"telemetry\"].latency_ms,\n",
    "        \"openai_input_tokens\": openai_result[\"telemetry\"].input_tokens,\n",
    "        \"openai_output_tokens\": openai_result[\"telemetry\"].output_tokens,\n",
    "    })\n",
    "\n",
    "# Calculate costs\n",
    "df = pd.DataFrame(results)\n",
    "df[\"bedrock_cost\"] = (df[\"bedrock_input_tokens\"] / 1000 * 0.003) + \\\n",
    "                     (df[\"bedrock_output_tokens\"] / 1000 * 0.015)\n",
    "df[\"openai_cost\"] = (df[\"openai_input_tokens\"] / 1000 * 0.005) + \\\n",
    "                    (df[\"openai_output_tokens\"] / 1000 * 0.015)\n",
    "\n",
    "# Summary statistics\n",
    "print(df.groupby(\"provider\").agg({\n",
    "    \"latency_ms\": [\"mean\", \"median\", \"std\"],\n",
    "    \"cost\": \"sum\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce87a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>bedrock_latency_ms</th>\n",
       "      <th>bedrock_input_tokens</th>\n",
       "      <th>bedrock_output_tokens</th>\n",
       "      <th>openai_latency_ms</th>\n",
       "      <th>openai_input_tokens</th>\n",
       "      <th>openai_output_tokens</th>\n",
       "      <th>bedrock_cost</th>\n",
       "      <th>openai_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain quantum computing in simple terms.</td>\n",
       "      <td>7061.64</td>\n",
       "      <td>8</td>\n",
       "      <td>512</td>\n",
       "      <td>5357.69</td>\n",
       "      <td>14</td>\n",
       "      <td>321</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.004885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a Python function to merge sorted lists.</td>\n",
       "      <td>7085.45</td>\n",
       "      <td>9</td>\n",
       "      <td>512</td>\n",
       "      <td>6277.78</td>\n",
       "      <td>16</td>\n",
       "      <td>454</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.006890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the last 5 years of AI research.</td>\n",
       "      <td>6259.55</td>\n",
       "      <td>12</td>\n",
       "      <td>436</td>\n",
       "      <td>7939.35</td>\n",
       "      <td>19</td>\n",
       "      <td>637</td>\n",
       "      <td>0.006576</td>\n",
       "      <td>0.009650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           prompt  bedrock_latency_ms  \\\n",
       "0      Explain quantum computing in simple terms.             7061.64   \n",
       "1  Write a Python function to merge sorted lists.             7085.45   \n",
       "2      Summarize the last 5 years of AI research.             6259.55   \n",
       "\n",
       "   bedrock_input_tokens  bedrock_output_tokens  openai_latency_ms  \\\n",
       "0                     8                    512            5357.69   \n",
       "1                     9                    512            6277.78   \n",
       "2                    12                    436            7939.35   \n",
       "\n",
       "   openai_input_tokens  openai_output_tokens  bedrock_cost  openai_cost  \n",
       "0                   14                   321      0.007704     0.004885  \n",
       "1                   16                   454      0.007707     0.006890  \n",
       "2                   19                   637      0.006576     0.009650  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "198c0985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: Explain quantum computing in simple terms....\n",
      "============================================================\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 7294.49ms\n",
      "  Response Length: 2659 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "\n",
      "============================================================\n",
      "Testing: Write a Python function to merge sorted lists....\n",
      "============================================================\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 7227.04ms\n",
      "  Response Length: 1829 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "\n",
      "============================================================\n",
      "Testing: Summarize the last 5 years of AI research....\n",
      "============================================================\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 7156.96ms\n",
      "  Response Length: 2814 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "\n",
      "============================================================\n",
      "Testing: What are the benefits of cloud computing?...\n",
      "============================================================\n",
      "✓ Model Invocation Successful\n",
      "  Latency: 6564.28ms\n",
      "  Response Length: 2164 characters\n",
      "✓ OpenAI Invocation Successful (gpt-4o)\n",
      "\n",
      "================================================================================\n",
      "EMPIRICAL BENCHMARKING RESULTS - SUMMARY STATISTICS\n",
      "================================================================================\n",
      "         latency_ms                                          input_tokens  \\\n",
      "               mean   median          std      min       max         mean   \n",
      "provider                                                                    \n",
      "bedrock   7060.6925  7192.00   335.671250  6564.28   7294.49         9.25   \n",
      "openai    9770.8325  9461.39  2798.436672  7320.34  12840.21        16.00   \n",
      "\n",
      "                output_tokens             cost            success  \n",
      "         median          mean median       sum      mean <lambda>  \n",
      "provider                                                           \n",
      "bedrock     8.5        477.75  512.0  0.028776  0.007194   100.0%  \n",
      "openai     15.5        547.50  529.0  0.033170  0.008292   100.0%  \n",
      "\n",
      "================================================================================\n",
      "COST COMPARISON\n",
      "================================================================================\n",
      "               sum      mean    median  total_tokens  cost_per_1k_tokens\n",
      "provider                                                                \n",
      "bedrock   0.028776  0.007194  0.007706          1948            0.014772\n",
      "openai    0.033170  0.008292  0.008013          2254            0.014716\n",
      "\n",
      "💰 Cost Savings: 13.2% savings using Bedrock vs OpenAI\n",
      "   Bedrock Total: $0.0288\n",
      "   OpenAI Total: $0.0332\n",
      "\n",
      "================================================================================\n",
      "LATENCY COMPARISON\n",
      "================================================================================\n",
      "               mean   median          std\n",
      "provider                                 \n",
      "bedrock   7060.6925  7192.00   335.671250\n",
      "openai    9770.8325  9461.39  2798.436672\n",
      "\n",
      "⚡ Speed: Bedrock is 27.7% faster than OpenAI\n",
      "\n",
      "✅ Results saved to 'empirical_benchmarking_results.csv'\n",
      "\n",
      "================================================================================\n",
      "SAMPLE RESULTS (First 5 prompts)\n",
      "================================================================================\n",
      "                                           prompt provider  latency_ms  \\\n",
      "0      Explain quantum computing in simple terms.  bedrock     7294.49   \n",
      "1      Explain quantum computing in simple terms.   openai    12840.21   \n",
      "2  Write a Python function to merge sorted lists.  bedrock     7227.04   \n",
      "3  Write a Python function to merge sorted lists.   openai     7476.92   \n",
      "4      Summarize the last 5 years of AI research.  bedrock     7156.96   \n",
      "5      Summarize the last 5 years of AI research.   openai    11445.86   \n",
      "6       What are the benefits of cloud computing?  bedrock     6564.28   \n",
      "7       What are the benefits of cloud computing?   openai     7320.34   \n",
      "\n",
      "   input_tokens  output_tokens      cost  \n",
      "0             8            512  0.007704  \n",
      "1            14            416  0.006310  \n",
      "2             9            512  0.007707  \n",
      "3            16            619  0.009365  \n",
      "4            12            512  0.007716  \n",
      "5            19            716  0.010835  \n",
      "6             8            375  0.005649  \n",
      "7            15            439  0.006660  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Musharraf\\AppData\\Local\\Temp\\ipykernel_20856\\3102612306.py:78: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cost_comparison[\"total_tokens\"] = df.groupby(\"provider\").apply(\n"
     ]
    }
   ],
   "source": [
    "# Run your Week 1 notebook with benchmark prompts\n",
    "benchmark_prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a Python function to merge sorted lists.\",\n",
    "    \"Summarize the last 5 years of AI research.\",\n",
    "    \"What are the benefits of cloud computing?\",\n",
    "]\n",
    "\n",
    "# Collect telemetry\n",
    "results = []\n",
    "for prompt in benchmark_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {prompt[:50]}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # AWS Bedrock\n",
    "    bedrock_result = invoke_llama_with_governance(prompt, MODEL_ID)\n",
    "    \n",
    "    # OpenAI\n",
    "    openai_result = invoke_openai_with_governance(prompt, model=\"gpt-4o\")\n",
    "    \n",
    "    # Add Bedrock result\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"provider\": \"bedrock\",\n",
    "        \"latency_ms\": bedrock_result[\"telemetry\"].latency_ms,\n",
    "        \"input_tokens\": bedrock_result[\"telemetry\"].input_tokens,\n",
    "        \"output_tokens\": bedrock_result[\"telemetry\"].output_tokens,\n",
    "        \"success\": bedrock_result[\"telemetry\"].success,\n",
    "    })\n",
    "    \n",
    "    # Add OpenAI result\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"provider\": \"openai\",\n",
    "        \"latency_ms\": openai_result[\"telemetry\"].latency_ms,\n",
    "        \"input_tokens\": openai_result[\"telemetry\"].input_tokens,\n",
    "        \"output_tokens\": openai_result[\"telemetry\"].output_tokens,\n",
    "        \"success\": openai_result[\"telemetry\"].success,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate costs (based on your research report pricing)\n",
    "# Bedrock (Meta Llama 3): $0.003/1K input, $0.015/1K output\n",
    "# OpenAI GPT-4o: $0.005/1K input, $0.015/1K output\n",
    "df[\"cost\"] = df.apply(\n",
    "    lambda row: (\n",
    "        (row[\"input_tokens\"] / 1000 * 0.003) + (row[\"output_tokens\"] / 1000 * 0.015)\n",
    "        if row[\"provider\"] == \"bedrock\"\n",
    "        else (row[\"input_tokens\"] / 1000 * 0.005) + (row[\"output_tokens\"] / 1000 * 0.015)\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EMPIRICAL BENCHMARKING RESULTS - SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = df.groupby(\"provider\").agg({\n",
    "    \"latency_ms\": [\"mean\", \"median\", \"std\", \"min\", \"max\"],\n",
    "    \"input_tokens\": [\"mean\", \"median\"],\n",
    "    \"output_tokens\": [\"mean\", \"median\"],\n",
    "    \"cost\": [\"sum\", \"mean\"],\n",
    "    \"success\": lambda x: f\"{(x.sum() / len(x)) * 100:.1f}%\"\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Detailed comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COST COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cost_comparison = df.groupby(\"provider\")[\"cost\"].agg([\"sum\", \"mean\", \"median\"])\n",
    "cost_comparison[\"total_tokens\"] = df.groupby(\"provider\").apply(\n",
    "    lambda x: (x[\"input_tokens\"] + x[\"output_tokens\"]).sum()\n",
    ")\n",
    "cost_comparison[\"cost_per_1k_tokens\"] = (cost_comparison[\"sum\"] / cost_comparison[\"total_tokens\"]) * 1000\n",
    "\n",
    "print(cost_comparison)\n",
    "\n",
    "# Calculate savings\n",
    "bedrock_total = cost_comparison.loc[\"bedrock\", \"sum\"]\n",
    "openai_total = cost_comparison.loc[\"openai\", \"sum\"]\n",
    "savings_pct = ((openai_total - bedrock_total) / openai_total) * 100\n",
    "\n",
    "print(f\"\\n💰 Cost Savings: {savings_pct:.1f}% savings using Bedrock vs OpenAI\")\n",
    "print(f\"   Bedrock Total: ${bedrock_total:.4f}\")\n",
    "print(f\"   OpenAI Total: ${openai_total:.4f}\")\n",
    "\n",
    "# Latency comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LATENCY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "latency_comparison = df.groupby(\"provider\")[\"latency_ms\"].agg([\"mean\", \"median\", \"std\"])\n",
    "print(latency_comparison)\n",
    "\n",
    "bedrock_latency = latency_comparison.loc[\"bedrock\", \"mean\"]\n",
    "openai_latency = latency_comparison.loc[\"openai\", \"mean\"]\n",
    "speed_diff = ((openai_latency - bedrock_latency) / openai_latency) * 100\n",
    "\n",
    "if bedrock_latency < openai_latency:\n",
    "    print(f\"\\n⚡ Speed: Bedrock is {abs(speed_diff):.1f}% faster than OpenAI\")\n",
    "else:\n",
    "    print(f\"\\n⚡ Speed: OpenAI is {abs(speed_diff):.1f}% faster than Bedrock\")\n",
    "\n",
    "# Export results\n",
    "df.to_csv(\"empirical_benchmarking_results.csv\", index=False)\n",
    "print(\"\\n✅ Results saved to 'empirical_benchmarking_results.csv'\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE RESULTS (First 5 prompts)\")\n",
    "print(\"=\"*80)\n",
    "print(df[[\"prompt\", \"provider\", \"latency_ms\", \"input_tokens\", \"output_tokens\", \"cost\"]].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
