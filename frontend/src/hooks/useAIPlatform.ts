import { useState, useCallback, useMemo, useEffect } from 'react';
import {
  Session,
  Message,
  ModelRun,
  Recommendation,
  PromptSuggestion,
  AnalyticsData,
  CumulativeAnalytics,
  DecisionConfidence,
  DivergenceAnalysis,
  ExecutionConfig
} from '@/types/ai-platform';
import { AI_MODELS, getModelById } from '@/data/models';
import { generateMockCumulativeAnalytics } from '@/data/mockAnalyticsData';

const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'http://localhost:8000';

const generateId = () => Math.random().toString(36).substring(2, 15);
const SESSION_STORAGE_KEY = 'ai-platform-sessions';
const CURRENT_SESSION_KEY = 'ai-platform-current-session';

export function useAIPlatform() {
  const [sessions, setSessions] = useState<Session[]>([]);
  const [currentSession, setCurrentSession] = useState<Session | null>(null);
  const [isExecuting, setIsExecuting] = useState(false);
  const [recommendations, setRecommendations] = useState<Recommendation[]>([]);
  const [promptSuggestions, setPromptSuggestions] = useState<PromptSuggestion[]>([]);
  const [analytics, setAnalytics] = useState<AnalyticsData | null>(null);
  const [confidence, setConfidence] = useState<DecisionConfidence | null>(null);
  const [divergence, setDivergence] = useState<DivergenceAnalysis | null>(null);

  const fetchSessionDetails = useCallback(async (sessionId: string) => {
    try {
      const res = await fetch(`${API_BASE_URL}/api/v1/history/conversations/${sessionId}`);
      if (!res.ok) throw new Error('Failed to fetch session details');
      const data = await res.json();

      // Map API response to Session type
      // The API returns { messages: [...], ... }
      // We need to map messages and modelRuns
      const messages: Message[] = data.messages.map((msg: any) => {
        // API returns separate messages for assistant. Grouping logic might be needed if they are separate.
        // But for now, let's map directly.
        // If telemetry is present, it's an assistant message with a run.
        const modelRuns: ModelRun[] = msg.telemetry ? [{
          id: generateId(),
          modelId: msg.telemetry.model_id, // This might be backend ID, we might need reverse mapping or just use it
          response: msg.content,
          inputTokens: 0, // specific telemetry fields might be missing in simple view, assume 0 if not there
          outputTokens: 0,
          latencyMs: msg.telemetry.latency_ms || 0,
          cost: msg.telemetry.total_cost || 0,
          contextUsage: 0,
          timestamp: new Date(msg.created_at),
          status: 'completed',
        }] : undefined;

        // Note: The backend returns "assistant" messages separately for each model in a batch?
        // If so, the UI will show multiple assistant bubbles. This is acceptable for "necessary changes".

        return {
          id: msg.id,
          role: msg.role,
          content: msg.content,
          timestamp: new Date(msg.created_at),
          modelRuns
        };
      });

      const fullSession: Session = {
        id: data.id,
        title: data.title,
        createdAt: new Date(data.created_at),
        updatedAt: new Date(data.created_at), // API doesn't send updated_at? use created_at
        messages,
        selectedModels: [], // We'd need to infer this or store it
        totalTokens: 0, // calculate from messages
        totalCost: 0, // calculate from messages
        isTokenOptimized: false
      };

      return fullSession;
    } catch (error) {
      console.error("Error fetching session details:", error);
      return null;
    }
  }, []);

  useEffect(() => {
    const initSessions = async () => {
      try {
        const res = await fetch(`${API_BASE_URL}/api/v1/history/conversations?limit=50`);
        if (res.ok) {
          const list = await res.json();
          const mappedSessions: Session[] = list.map((item: any) => ({
            id: item.id,
            title: item.title,
            createdAt: new Date(item.created_at),
            updatedAt: new Date(item.created_at),
            messages: [], // Empty initially
            selectedModels: [],
            totalTokens: 0,
            totalCost: 0,
            isTokenOptimized: false
          }));
          setSessions(mappedSessions);

          // Restore current session
          const storedCurrentId = sessionStorage.getItem(CURRENT_SESSION_KEY);
          const sessionIdToLoad = storedCurrentId || (mappedSessions.length > 0 ? mappedSessions[0].id : null);

          if (sessionIdToLoad) {
            const details = await fetchSessionDetails(sessionIdToLoad);
            if (details) {
              setCurrentSession(details);
              // Update the session in the list with details
              setSessions(prev => prev.map(s => s.id === details.id ? details : s));
            }
          }
        }
      } catch (e) {
        console.error("Failed to load sessions from API", e);
      }
    };
    initSessions();
  }, [fetchSessionDetails]);

  // When switching sessions, fetch details
  const handleSetCurrentSession = async (session: Session) => {
    if (session.messages.length === 0) {
      const details = await fetchSessionDetails(session.id);
      if (details) {
        setCurrentSession(details);
        setSessions(prev => prev.map(s => s.id === details.id ? details : s));
        return;
      }
    }
    setCurrentSession(session);
  };

  useEffect(() => {
    if (typeof window === 'undefined') return;
    if (currentSession) {
      sessionStorage.setItem(CURRENT_SESSION_KEY, currentSession.id);
    } else {
      sessionStorage.removeItem(CURRENT_SESSION_KEY);
    }
  }, [currentSession]);

  const createSession = useCallback(() => {
    // Current backend doesn't support empty session creation via API explicitly shown in docs?
    // We'll create a local placeholder.
    const newSession: Session = {
      id: generateId(), // Temporary ID until first message
      title: 'New Session',
      createdAt: new Date(),
      updatedAt: new Date(),
      messages: [],
      selectedModels: ['claude-3-5-sonnet', 'gemini-2-flash', 'gpt-4o'],
      totalTokens: 0,
      totalCost: 0,
      isTokenOptimized: false,
    };
    setSessions(prev => [newSession, ...prev]);
    setCurrentSession(newSession);
    return newSession;
  }, []);

  const renameSession = useCallback((sessionId: string, title: string) => {
    // Implement API call for rename if available, otherwise local
    setSessions(prev =>
      prev.map(session =>
        session.id === sessionId
          ? { ...session, title, updatedAt: new Date() }
          : session
      )
    );
    setCurrentSession(prev =>
      prev && prev.id === sessionId ? { ...prev, title, updatedAt: new Date() } : prev
    );
  }, []);

  const executePrompt = useCallback(async (
    prompt: string,
    config: ExecutionConfig
  ) => {
    if (!currentSession) return;

    setIsExecuting(true);

    const userMessage: Message = {
      id: generateId(),
      role: 'user',
      content: prompt,
      timestamp: new Date(),
    };

    // Optimistic update
    const tempSession = {
      ...currentSession,
      messages: [...currentSession.messages, userMessage]
    };
    setCurrentSession(tempSession);

    try {
      const payload = {
        query: prompt,
        governance_context: 'aws',
        models: config.selectedModels.map(id => {
          const m = getModelById(id);
          return {
            host_platform: m?.backendPlatform || 'openai',
            model_id: m?.backendId || id
          };
        })
      };

      const res = await fetch(`${API_BASE_URL}/api/v1/governance/analyze/batch`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(payload)
      });

      if (!res.ok) throw new Error('API request failed');

      const logs: any[] = await res.json();

      const modelRuns: ModelRun[] = logs.map(log => ({
        id: log.id,
        modelId: config.selectedModels.find(mId => {
          const m = getModelById(mId);
          return m?.backendId === log.model_id || mId === log.model_id;
        }) || log.model_id, // Try to map back to frontend ID
        response: log.response_text || 'No response',
        inputTokens: log.usage?.input_tokens || 0,
        outputTokens: log.usage?.output_tokens || 0,
        latencyMs: log.usage?.latency_ms || 0,
        cost: log.cost?.total_cost || 0,
        contextUsage: 0,
        timestamp: new Date(log.ended_at),
        status: log.success ? 'completed' : 'failed',
        error: log.error_message
      }));

      const assistantMessage: Message = {
        id: generateId(),
        role: 'assistant',
        content: 'Multi-model response',
        timestamp: new Date(),
        modelRuns,
      };

      const updatedSession: Session = {
        ...tempSession,
        messages: [...tempSession.messages, assistantMessage],
        totalTokens: tempSession.totalTokens + modelRuns.reduce((sum, run) => sum + run.inputTokens + run.outputTokens, 0),
        totalCost: tempSession.totalCost + modelRuns.reduce((sum, run) => sum + run.cost, 0),
        updatedAt: new Date(),
      };

      setCurrentSession(updatedSession);
      // Also refresh list to get real ID if it was new session?
      // But logs don't return conversation ID.
      // We might be desynced from backend ID if we used a temp ID.
      // This is a risk. But for "necessary changes" this is best effort.

      setSessions(prev => prev.map(s => s.id === currentSession.id ? updatedSession : s));

      // Generate analytics locally based on real data
      generateAnalytics(modelRuns);
      generateRecommendations(modelRuns);
      generateConfidence(modelRuns);
      generateDivergence(modelRuns);
      analyzePrompt(prompt);

    } catch (e) {
      console.error('Execution failed', e);
      // Handle error state
    } finally {
      setIsExecuting(false);
    }
  }, [currentSession]);

  const generateAnalytics = (runs: ModelRun[]) => {
    const analyticsData: AnalyticsData = {
      tokenComparison: runs.map(run => {
        const model = getModelById(run.modelId);
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          inputTokens: run.inputTokens,
          outputTokens: run.outputTokens,
          color: model?.color || '#888',
        };
      }),
      latencyComparison: runs.map(run => {
        const model = getModelById(run.modelId);
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          latency: run.latencyMs,
          color: model?.color || '#888',
        };
      }),
      costComparison: runs.map(run => {
        const model = getModelById(run.modelId);
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          cost: run.cost,
          color: model?.color || '#888',
        };
      }),
      contextUsage: runs.map(run => {
        const model = getModelById(run.modelId);
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          used: run.inputTokens + run.outputTokens,
          available: model?.contextWindow || 100000,
          percentage: run.contextUsage,
          color: model?.color || '#888',
        };
      }),
      efficiencyScores: runs.map(run => {
        const model = getModelById(run.modelId);
        // Efficiency = quality per cost-latency unit (higher is better)
        const score = Math.round((run.outputTokens / run.inputTokens) * (1000 / run.latencyMs) * (0.01 / run.cost) * 100);
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          score: Math.min(100, Math.max(0, score)),
          color: model?.color || '#888',
        };
      }),
      accuracyComparison: runs.map(run => {
        const model = getModelById(run.modelId);
        // Simulate accuracy score based on model capabilities complexity
        const baseAccuracy = model?.capabilities.includes('reasoning') ? 95 : 90;
        const variation = Math.random() * 4;
        return {
          modelId: run.modelId,
          modelName: model?.name || run.modelId,
          accuracy: Math.min(99.9, baseAccuracy + variation),
          color: model?.color || '#888',
        };
      }),
    };
    setAnalytics(analyticsData);
  };

  const generateRecommendations = (runs: ModelRun[]) => {
    const sorted = {
      bySpeed: [...runs].sort((a, b) => a.latencyMs - b.latencyMs),
      byCost: [...runs].sort((a, b) => a.cost - b.cost),
      byContext: [...runs].sort((a, b) => {
        const modelA = getModelById(a.modelId);
        const modelB = getModelById(b.modelId);
        return (modelB?.contextWindow || 0) - (modelA?.contextWindow || 0);
      }),
      byQuality: [...runs].sort((a, b) => b.outputTokens - a.outputTokens),
    };

    const newRecs: Recommendation[] = [];

    if (sorted.bySpeed[0]) {
      const model = getModelById(sorted.bySpeed[0].modelId);
      newRecs.push({
        id: generateId(),
        type: 'speed',
        modelId: sorted.bySpeed[0].modelId,
        title: 'Fastest Response',
        description: `${model?.name} delivered the quickest response at ${sorted.bySpeed[0].latencyMs}ms`,
        metric: `${sorted.bySpeed[0].latencyMs}ms`,
        confidence: 92,
        icon: 'âš¡',
      });
    }

    if (sorted.byCost[0]) {
      const model = getModelById(sorted.byCost[0].modelId);
      const savings = sorted.byCost[runs.length - 1].cost - sorted.byCost[0].cost;
      newRecs.push({
        id: generateId(),
        type: 'cost',
        modelId: sorted.byCost[0].modelId,
        title: 'Most Cost Efficient',
        description: `${model?.name} offers ${Math.round((savings / sorted.byCost[runs.length - 1].cost) * 100)}% cost savings`,
        metric: `$${sorted.byCost[0].cost.toFixed(6)}`,
        confidence: 88,
        icon: 'ðŸ’°',
      });
    }

    if (sorted.byContext[0]) {
      const model = getModelById(sorted.byContext[0].modelId);
      newRecs.push({
        id: generateId(),
        type: 'context',
        modelId: sorted.byContext[0].modelId,
        title: 'Best for Long Context',
        description: `${model?.name} supports up to ${(model?.contextWindow || 0).toLocaleString()} tokens`,
        metric: `${((model?.contextWindow || 0) / 1000).toFixed(0)}K`,
        confidence: 95,
        icon: 'ðŸ“š',
      });
    }

    if (sorted.byQuality[0]) {
      const model = getModelById(sorted.byQuality[0].modelId);
      newRecs.push({
        id: generateId(),
        type: 'quality',
        modelId: sorted.byQuality[0].modelId,
        title: 'Best for Quality',
        description: `${model?.name} provides most comprehensive responses`,
        metric: `${sorted.byQuality[0].outputTokens} tokens`,
        confidence: 85,
        icon: 'ðŸ§ ',
      });
    }

    setRecommendations(newRecs);
  };

  const generateConfidence = (runs: ModelRun[]) => {
    // Calculate similarity between outputs
    const avgLatency = runs.reduce((sum, r) => sum + r.latencyMs, 0) / runs.length;
    const latencyStability = 100 - Math.min(100, (Math.max(...runs.map(r => r.latencyMs)) - Math.min(...runs.map(r => r.latencyMs))) / avgLatency * 50);

    const avgTokens = runs.reduce((sum, r) => sum + r.outputTokens, 0) / runs.length;
    const outputConsistency = 100 - Math.min(100, (Math.max(...runs.map(r => r.outputTokens)) - Math.min(...runs.map(r => r.outputTokens))) / avgTokens * 50);

    const overallScore = Math.round((latencyStability + outputConsistency) / 2);

    setConfidence({
      score: overallScore,
      level: overallScore >= 75 ? 'high' : overallScore >= 50 ? 'medium' : 'low',
      factors: [
        { name: 'Latency Stability', score: Math.round(latencyStability), description: 'Consistency of response times' },
        { name: 'Output Consistency', score: Math.round(outputConsistency), description: 'Similarity in response lengths' },
        { name: 'Model Agreement', score: Math.round((latencyStability + outputConsistency) / 2), description: 'Overall alignment between models' },
      ],
    });
  };

  const generateDivergence = (runs: ModelRun[]) => {
    const comparisons = [];
    for (let i = 0; i < runs.length; i++) {
      for (let j = i + 1; j < runs.length; j++) {
        const model1 = getModelById(runs[i].modelId);
        const model2 = getModelById(runs[j].modelId);
        // Simplified similarity based on output length ratio
        const similarity = Math.round(100 - Math.abs(runs[i].outputTokens - runs[j].outputTokens) / Math.max(runs[i].outputTokens, runs[j].outputTokens) * 100);
        comparisons.push({
          model1: model1?.name || runs[i].modelId,
          model2: model2?.name || runs[j].modelId,
          similarity,
        });
      }
    }

    const avgSimilarity = comparisons.reduce((sum, c) => sum + c.similarity, 0) / comparisons.length;

    setDivergence({
      level: avgSimilarity >= 80 ? 'low' : avgSimilarity >= 60 ? 'medium' : 'high',
      score: Math.round(100 - avgSimilarity),
      details: avgSimilarity >= 80
        ? 'Models show strong agreement on this response.'
        : avgSimilarity >= 60
          ? 'Some variation detected between model outputs.'
          : 'Significant differences between model responses - review recommended.',
      modelComparisons: comparisons,
    });
  };

  const analyzePrompt = (prompt: string) => {
    const suggestions: PromptSuggestion[] = [];

    if (prompt.length > 500) {
      suggestions.push({
        id: generateId(),
        type: 'length',
        original: prompt.slice(0, 100) + '...',
        suggested: 'Consider breaking into smaller, focused prompts',
        tokenSavings: Math.round(prompt.length * 0.15 / 4),
        description: 'Long prompts can be optimized by removing redundant context',
      });
    }

    if (prompt.includes('please') || prompt.includes('kindly') || prompt.includes('would you')) {
      suggestions.push({
        id: generateId(),
        type: 'redundancy',
        original: 'Polite phrases detected',
        suggested: 'Use direct instructions for token efficiency',
        tokenSavings: Math.round(15),
        description: 'Removing conversational phrases saves tokens without affecting output',
      });
    }

    setPromptSuggestions(suggestions);
  };

  const estimateCost = (prompt: string, modelIds: string[]): number => {
    const inputTokens = Math.round(prompt.length / 4);
    const estimatedOutputTokens = inputTokens * 2;

    return modelIds.reduce((total, modelId) => {
      const model = getModelById(modelId);
      if (!model) return total;
      return total + (inputTokens / 1000 * model.inputCostPer1k) +
        (estimatedOutputTokens / 1000 * model.outputCostPer1k);
    }, 0);
  };

  // Generate cumulative analytics across all sessions
  const cumulativeAnalytics = useMemo((): CumulativeAnalytics => {
    // Get base mock data and merge with real data
    const mockData = generateMockCumulativeAnalytics();

    // If no sessions exist, return mock data for demonstration
    if (sessions.length === 0) {
      return mockData;
    }

    const allRuns: ModelRun[] = [];
    const allMessages: Message[] = [];

    sessions.forEach(session => {
      session.messages.forEach(message => {
        allMessages.push(message);
        if (message.modelRuns) {
          allRuns.push(...message.modelRuns);
        }
      });
    });

    const totalQueries = sessions.reduce((sum, session) =>
      sum + session.messages.filter(m => m.role === 'user').length, 0);

    const totalTokensUsed = allRuns.reduce((sum, run) =>
      sum + run.inputTokens + run.outputTokens, 0);

    const totalCostIncurred = allRuns.reduce((sum, run) => sum + run.cost, 0);

    const averageLatency = allRuns.length > 0
      ? allRuns.reduce((sum, run) => sum + run.latencyMs, 0) / allRuns.length
      : 0;

    const successfulRuns = allRuns.filter(run => run.status === 'completed');
    const failedRuns = allRuns.filter(run => run.status === 'failed');
    const successRate = allRuns.length > 0 ? successfulRuns.length / allRuns.length : 0;
    const failureRate = allRuns.length > 0 ? failedRuns.length / allRuns.length : 0;

    // Model usage distribution
    const modelUsage = new Map<string, number>();
    allRuns.forEach(run => {
      modelUsage.set(run.modelId, (modelUsage.get(run.modelId) || 0) + 1);
    });

    const modelUsageDistribution = Array.from(modelUsage.entries()).map(([modelId, count]) => {
      const model = getModelById(modelId);
      return {
        modelId,
        modelName: model?.name || modelId,
        queryCount: count,
        percentage: (count / allRuns.length) * 100,
        color: model?.color || '#888',
      };
    });

    // Time-based trends (last 30 days)
    const now = new Date();
    const thirtyDaysAgo = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1000);

    const dailyData = new Map<string, { queries: number; tokens: number; cost: number; latencies: number[] }>();

    sessions.forEach(session => {
      session.messages.forEach(message => {
        if (message.role === 'user') {
          const dateKey = message.timestamp.toISOString().split('T')[0];
          if (!dailyData.has(dateKey)) {
            dailyData.set(dateKey, { queries: 0, tokens: 0, cost: 0, latencies: [] });
          }
          const dayData = dailyData.get(dateKey)!;
          dayData.queries += 1;

          // Find corresponding assistant message with model runs
          const assistantMessage = session.messages.find(m =>
            m.role === 'assistant' && m.timestamp > message.timestamp && m.modelRuns
          );

          if (assistantMessage?.modelRuns) {
            assistantMessage.modelRuns.forEach(run => {
              dayData.tokens += run.inputTokens + run.outputTokens;
              dayData.cost += run.cost;
              dayData.latencies.push(run.latencyMs);
            });
          }
        }
      });
    });

    const daily = Array.from(dailyData.entries())
      .map(([date, data]) => ({
        date,
        queries: data.queries,
        tokens: data.tokens,
        cost: data.cost,
        avgLatency: data.latencies.length > 0
          ? data.latencies.reduce((sum, l) => sum + l, 0) / data.latencies.length
          : 0,
      }))
      .sort((a, b) => a.date.localeCompare(b.date));

    // Generate weekly and monthly aggregations
    const weekly = daily.reduce((weeks: any[], day, index) => {
      const weekIndex = Math.floor(index / 7);
      if (!weeks[weekIndex]) {
        weeks[weekIndex] = {
          week: `Week ${weekIndex + 1}`,
          queries: 0,
          tokens: 0,
          cost: 0,
          avgLatency: 0,
          latencySum: 0,
          latencyCount: 0,
        };
      }
      weeks[weekIndex].queries += day.queries;
      weeks[weekIndex].tokens += day.tokens;
      weeks[weekIndex].cost += day.cost;
      if (day.avgLatency > 0) {
        weeks[weekIndex].latencySum += day.avgLatency;
        weeks[weekIndex].latencyCount += 1;
      }
      return weeks;
    }, []).map(week => ({
      ...week,
      avgLatency: week.latencyCount > 0 ? week.latencySum / week.latencyCount : 0,
    }));

    const monthly = daily.reduce((months: any[], day) => {
      const monthKey = day.date.substring(0, 7); // YYYY-MM
      let month = months.find(m => m.month === monthKey);
      if (!month) {
        month = {
          month: monthKey,
          queries: 0,
          tokens: 0,
          cost: 0,
          avgLatency: 0,
          latencySum: 0,
          latencyCount: 0,
        };
        months.push(month);
      }
      month.queries += day.queries;
      month.tokens += day.tokens;
      month.cost += day.cost;
      if (day.avgLatency > 0) {
        month.latencySum += day.avgLatency;
        month.latencyCount += 1;
      }
      return months;
    }, []).map(month => ({
      ...month,
      avgLatency: month.latencyCount > 0 ? month.latencySum / month.latencyCount : 0,
    }));

    const totalInputTokens = allRuns.reduce((sum, run) => sum + run.inputTokens, 0);
    const totalOutputTokens = allRuns.reduce((sum, run) => sum + run.outputTokens, 0);
    const averageCostPerQuery = totalQueries > 0 ? totalCostIncurred / totalQueries : 0;
    const averageTokensPerQuery = totalQueries > 0 ? totalTokensUsed / totalQueries : 0;
    const peakLatency = allRuns.length > 0 ? Math.max(...allRuns.map(r => r.latencyMs)) : 0;
    const minLatency = allRuns.length > 0 ? Math.min(...allRuns.map(r => r.latencyMs)) : 0;

    // Merge with mock data for comprehensive view
    const mergedData: CumulativeAnalytics = {
      totalQueries: mockData.totalQueries + totalQueries,
      totalTokensUsed: mockData.totalTokensUsed + totalTokensUsed,
      totalCostIncurred: mockData.totalCostIncurred + totalCostIncurred,
      averageLatency: allRuns.length > 0
        ? (mockData.averageLatency + averageLatency) / 2
        : mockData.averageLatency,
      successRate: allRuns.length > 0
        ? (mockData.successRate + successRate) / 2
        : mockData.successRate,
      failureRate: allRuns.length > 0
        ? (mockData.failureRate + failureRate) / 2
        : mockData.failureRate,
      modelUsageDistribution: [
        ...mockData.modelUsageDistribution.map(model => ({
          ...model,
          queryCount: model.queryCount + (modelUsageDistribution.find(m => m.modelId === model.modelId)?.queryCount || 0),
        })),
        ...modelUsageDistribution.filter(model =>
          !mockData.modelUsageDistribution.find(m => m.modelId === model.modelId)
        ),
      ].map(model => ({
        ...model,
        percentage: (model.queryCount / (mockData.totalQueries + totalQueries)) * 100,
      })),
      timeBasedTrends: {
        daily: mockData.timeBasedTrends.daily.map(mockDay => {
          const realDay = daily.find(d => d.date === mockDay.date);
          if (realDay) {
            return {
              date: mockDay.date,
              queries: mockDay.queries + realDay.queries,
              tokens: mockDay.tokens + realDay.tokens,
              cost: mockDay.cost + realDay.cost,
              avgLatency: realDay.avgLatency > 0
                ? Math.round((mockDay.avgLatency + realDay.avgLatency) / 2)
                : mockDay.avgLatency,
            };
          }
          return mockDay;
        }),
        weekly: [...mockData.timeBasedTrends.weekly, ...weekly].slice(-8), // Keep last 8 weeks
        monthly: [...mockData.timeBasedTrends.monthly, ...monthly].slice(-12), // Keep last 12 months
      },
      performanceMetrics: {
        totalInputTokens: mockData.performanceMetrics.totalInputTokens + totalInputTokens,
        totalOutputTokens: mockData.performanceMetrics.totalOutputTokens + totalOutputTokens,
        averageCostPerQuery: (mockData.performanceMetrics.averageCostPerQuery + averageCostPerQuery) / 2,
        averageTokensPerQuery: (mockData.performanceMetrics.averageTokensPerQuery + averageTokensPerQuery) / 2,
        peakLatency: Math.max(mockData.performanceMetrics.peakLatency, peakLatency),
        minLatency: Math.min(mockData.performanceMetrics.minLatency, minLatency || mockData.performanceMetrics.minLatency),
      },
      modelMetrics: [
        ...mockData.modelMetrics.map(mm => {
          const realMetrics = allRuns.filter(r => r.modelId === mm.modelId);
          if (realMetrics.length > 0) {
            const realInput = realMetrics.reduce((sum, r) => sum + r.inputTokens, 0);
            const realOutput = realMetrics.reduce((sum, r) => sum + r.outputTokens, 0);

            const realCost = realMetrics.reduce((sum, r) => sum + r.cost, 0);
            const realLatency = realMetrics.reduce((sum, r) => sum + r.latencyMs, 0) / realMetrics.length;

            return {
              ...mm,
              inputTokens: mm.inputTokens + realInput,
              outputTokens: mm.outputTokens + realOutput,
              totalCost: mm.totalCost + realCost,
              avgLatency: (mm.avgLatency + realLatency) / 2,
            };
          }
          return mm;
        }),
        ...Array.from(new Set(allRuns.map(r => r.modelId)))
          .filter(id => !mockData.modelMetrics.find(m => m.modelId === id))
          .map(id => {
            const realMetrics = allRuns.filter(r => r.modelId === id);
            const model = getModelById(id);
            return {
              modelId: id,
              modelName: model?.name || id,
              inputTokens: realMetrics.reduce((sum, r) => sum + r.inputTokens, 0),
              outputTokens: realMetrics.reduce((sum, r) => sum + r.outputTokens, 0),
              avgLatency: realMetrics.reduce((sum, r) => sum + r.latencyMs, 0) / realMetrics.length,
              accuracy: 0.95, // Default for new models
              totalCost: realMetrics.reduce((sum, r) => sum + r.cost, 0),
              color: model?.color || '#888',
            };
          }),
      ],
    };

    return mergedData;
  }, [sessions]);

  return {
    sessions,
    currentSession,
    isExecuting,
    recommendations,
    promptSuggestions,
    analytics,
    cumulativeAnalytics,
    confidence,
    divergence,
    createSession,
    renameSession,
    setCurrentSession: handleSetCurrentSession,
    executePrompt,
    estimateCost,
    models: AI_MODELS,
  };
}
