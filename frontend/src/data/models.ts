import { AIModel } from '@/types/ai-platform';

export const AI_MODELS: AIModel[] = [
  // AWS Bedrock - Anthropic Claude
  {
    id: 'anthropic.claude-sonnet-4-20250514-v1:0',
    name: 'Claude 3.5 Sonnet',
    provider: 'anthropic',
    hostPlatform: 'aws_bedrock',
    description: 'Most intelligent model, best for complex reasoning',
    contextWindow: 200000,
    inputCostPer1k: 0.003,
    outputCostPer1k: 0.015,
    avgLatency: 2500,
    capabilities: ['reasoning', 'code', 'analysis', 'creative'],
    color: '#D97706',
  },
  {
    id: 'anthropic.claude-3-sonnet-20240229-v1:0',
    name: 'Claude 3 Sonnet',
    provider: 'anthropic',
    hostPlatform: 'aws_bedrock',
    description: 'Balanced Claude model for general tasks',
    contextWindow: 200000,
    inputCostPer1k: 0.003,
    outputCostPer1k: 0.015,
    avgLatency: 2300,
    capabilities: ['reasoning', 'code', 'analysis'],
    color: '#F59E0B',
  },
  {
    id: 'anthropic.claude-3-haiku-20240307-v1:0',
    name: 'Claude 3 Haiku',
    provider: 'anthropic',
    hostPlatform: 'aws_bedrock',
    description: 'Fastest Claude model, great for simple tasks',
    contextWindow: 200000,
    inputCostPer1k: 0.00025,
    outputCostPer1k: 0.00125,
    avgLatency: 800,
    capabilities: ['speed', 'simple-tasks', 'classification'],
    color: '#FCD34D',
  },

  // GCP Vertex AI - Gemini
  {
    id: 'gemini-2.5-pro',
    name: 'Gemini 2.5 Pro',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Strongest quality for code & complex prompts',
    contextWindow: 2000000,
    inputCostPer1k: 0.00125,
    outputCostPer1k: 0.005,
    avgLatency: 2000,
    capabilities: ['reasoning', 'code', 'long-context', 'multimodal'],
    color: '#34A853',
  },
  {
    id: 'gemini-2.5-flash',
    name: 'Gemini 2.5 Flash',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Best for balancing reasoning and speed',
    contextWindow: 1000000,
    inputCostPer1k: 0.000075,
    outputCostPer1k: 0.0003,
    avgLatency: 600,
    capabilities: ['multimodal', 'speed', 'long-context'],
    color: '#4285F4',
  },
  {
    id: 'gemini-2.5-flash-lite',
    name: 'Gemini 2.5 Flash Lite',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Most balanced for low latency use cases',
    contextWindow: 1000000,
    inputCostPer1k: 0.0000375,
    outputCostPer1k: 0.00015,
    avgLatency: 400,
    capabilities: ['speed', 'low-latency'],
    color: '#8AB4F8',
  },
  {
    id: 'gemini-3-pro-preview',
    name: 'Gemini 3 Pro Preview',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Most powerful agentic and coding model',
    contextWindow: 2000000,
    inputCostPer1k: 0.0025,
    outputCostPer1k: 0.01,
    avgLatency: 2500,
    capabilities: ['reasoning', 'code', 'agentic', 'long-context', 'multimodal'],
    color: '#34A853',
  },
  {
    id: 'gemini-3-flash-preview',
    name: 'Gemini 3 Flash Preview',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Agentic workhorse model',
    contextWindow: 1000000,
    inputCostPer1k: 0.000075,
    outputCostPer1k: 0.0003,
    avgLatency: 800,
    capabilities: ['agentic', 'multimodal', 'speed', 'long-context'],
    color: '#4285F4',
  },
  {
    id: 'gemini-3-pro-image-preview',
    name: 'Gemini 3 Pro Image Preview',
    provider: 'google',
    hostPlatform: 'gcp_vertex',
    description: 'Creative workflows with image generation',
    contextWindow: 2000000,
    inputCostPer1k: 0.00125,
    outputCostPer1k: 0.005,
    avgLatency: 3000,
    capabilities: ['image-generation', 'creative', 'multimodal', 'long-context'],
    color: '#1A73E8',
  },

  // OpenAI
  {
    id: 'gpt-4o',
    name: 'GPT-4o',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'OpenAI flagship model, balanced performance',
    contextWindow: 128000,
    inputCostPer1k: 0.0025,
    outputCostPer1k: 0.01,
    avgLatency: 1800,
    capabilities: ['reasoning', 'code', 'creative', 'vision'],
    color: '#10B981',
  },
  {
    id: 'gpt-4o-mini',
    name: 'GPT-4o Mini',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Cost-effective for straightforward tasks',
    contextWindow: 128000,
    inputCostPer1k: 0.00015,
    outputCostPer1k: 0.0006,
    avgLatency: 700,
    capabilities: ['speed', 'cost-effective', 'simple-tasks'],
    color: '#059669',
  },
  {
    id: 'o1',
    name: 'O1',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Advanced reasoning model',
    contextWindow: 128000,
    inputCostPer1k: 0.015,
    outputCostPer1k: 0.06,
    avgLatency: 3000,
    capabilities: ['reasoning', 'complex-problems'],
    color: '#047857',
  },
  {
    id: 'gpt-5.2',
    name: 'GPT-5.2',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Latest and most capable GPT-5 model',
    contextWindow: 256000,
    inputCostPer1k: 0.00175,
    outputCostPer1k: 0.014,
    avgLatency: 2200,
    capabilities: ['reasoning', 'code', 'creative', 'vision', 'advanced-analysis'],
    color: '#10B981',
  },
  {
    id: 'gpt-5.1',
    name: 'GPT-5.1',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Mid-tier GPT-5 model with excellent performance',
    contextWindow: 256000,
    inputCostPer1k: 0.00125,
    outputCostPer1k: 0.01,
    avgLatency: 2000,
    capabilities: ['reasoning', 'code', 'creative', 'vision'],
    color: '#059669',
  },
  {
    id: 'gpt-5',
    name: 'GPT-5',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Base GPT-5 model, balanced performance',
    contextWindow: 256000,
    inputCostPer1k: 0.00125,
    outputCostPer1k: 0.01,
    avgLatency: 2000,
    capabilities: ['reasoning', 'code', 'creative', 'vision'],
    color: '#047857',
  },
  {
    id: 'gpt-5-mini',
    name: 'GPT-5 Mini',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Cost-effective GPT-5 variant for most tasks',
    contextWindow: 256000,
    inputCostPer1k: 0.00025,
    outputCostPer1k: 0.002,
    avgLatency: 1200,
    capabilities: ['reasoning', 'code', 'speed', 'cost-effective'],
    color: '#34D399',
  },
  {
    id: 'gpt-5-nano',
    name: 'GPT-5 Nano',
    provider: 'openai',
    hostPlatform: 'openai',
    description: 'Ultra-fast, lightweight GPT-5 for simple tasks',
    contextWindow: 128000,
    inputCostPer1k: 0.00005,
    outputCostPer1k: 0.0004,
    avgLatency: 600,
    capabilities: ['speed', 'cost-effective', 'simple-tasks'],
    color: '#6EE7B7',
  },

  // AWS Bedrock - Meta Llama
  {
    id: 'meta.llama3-70b-instruct-v1:0',
    name: 'Llama 3 70B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Open-source model via AWS Bedrock',
    contextWindow: 8000,
    inputCostPer1k: 0.00099,
    outputCostPer1k: 0.00099,
    avgLatency: 1500,
    capabilities: ['reasoning', 'open-source'],
    color: '#FF9900',
  },
  {
    id: 'us.meta.llama4-maverick-17b-instruct-v1:0',
    name: 'Llama 4 Maverick 17B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Latest Llama 4 model with enhanced capabilities',
    contextWindow: 128000,
    inputCostPer1k: 0.00024,
    outputCostPer1k: 0.00097,
    avgLatency: 1200,
    capabilities: ['reasoning', 'code', 'open-source', 'long-context'],
    color: '#FF9900',
  },
  {
    id: 'us.meta.llama4-scout-17b-instruct-v1:0',
    name: 'Llama 4 Scout 17B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Efficient Llama 4 variant for fast inference',
    contextWindow: 128000,
    inputCostPer1k: 0.00017,
    outputCostPer1k: 0.00066,
    avgLatency: 1000,
    capabilities: ['speed', 'reasoning', 'open-source', 'long-context'],
    color: '#FFB84D',
  },
  {
    id: 'us.meta.llama3-3-70b-instruct-v1:0',
    name: 'Llama 3.3 Instruct 70B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Latest Llama 3.3 with improved instruction following',
    contextWindow: 128000,
    inputCostPer1k: 0.00072,
    outputCostPer1k: 0.00072,
    avgLatency: 1400,
    capabilities: ['reasoning', 'instruction-following', 'open-source'],
    color: '#FF9900',
  },
  {
    id: 'us.meta.llama3-2-1b-instruct-v1:0',
    name: 'Llama 3.2 Instruct 1B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Ultra-lightweight Llama for edge deployment',
    contextWindow: 128000,
    inputCostPer1k: 0.0001,
    outputCostPer1k: 0.0001,
    avgLatency: 300,
    capabilities: ['speed', 'lightweight', 'open-source', 'edge'],
    color: '#FFCC80',
  },
  {
    id: 'us.meta.llama3-2-3b-instruct-v1:0',
    name: 'Llama 3.2 Instruct 3B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Lightweight Llama for efficient inference',
    contextWindow: 128000,
    inputCostPer1k: 0.00015,
    outputCostPer1k: 0.00015,
    avgLatency: 400,
    capabilities: ['speed', 'lightweight', 'open-source'],
    color: '#FFCC80',
  },
  {
    id: 'us.meta.llama3-2-11b-instruct-v1:0',
    name: 'Llama 3.2 Instruct 11B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Mid-size Llama balancing performance and efficiency',
    contextWindow: 128000,
    inputCostPer1k: 0.00016,
    outputCostPer1k: 0.00016,
    avgLatency: 800,
    capabilities: ['reasoning', 'balanced', 'open-source'],
    color: '#FFB84D',
  },
  {
    id: 'us.meta.llama3-2-90b-instruct-v1:0',
    name: 'Llama 3.2 Instruct 90B',
    provider: 'amazon',
    hostPlatform: 'aws_bedrock',
    description: 'Large Llama model for complex reasoning tasks',
    contextWindow: 128000,
    inputCostPer1k: 0.00072,
    outputCostPer1k: 0.00072,
    avgLatency: 1600,
    capabilities: ['reasoning', 'complex-tasks', 'open-source'],
    color: '#FF9900',
  },
];

export const getModelById = (id: string): AIModel | undefined => {
  return AI_MODELS.find(model => model.id === id);
};

export const getModelsByProvider = (provider: string): AIModel[] => {
  return AI_MODELS.filter(model => model.provider === provider);
};

export const getModelsByPlatform = (platform: string): AIModel[] => {
  return AI_MODELS.filter(model => model.hostPlatform === platform);
};
